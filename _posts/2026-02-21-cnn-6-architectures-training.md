---
layout: post
title: "CNN 6강 - CNN 아키텍처와 학습 기법"
description: "Normalization, Dropout, VGGNet, ResNet, Transfer Learning, Data Augmentation"
date: 2026-02-21
category: "Deep Learning"
subcategory: "CNN"
tags: deep-learning, cnn, resnet, vggnet, transfer-learning
comments: true
---

## 잘한 점

### 상황 1.
- **상황**: 다양한 CNN 아키텍처(VGGNet, ResNet)를 비교 학습했다
- **액션**: 각 아키텍처의 핵심 아이디어와 구조적 차이를 정리했다
- **칭찬**: 단순 암기가 아니라 왜 그런 구조가 나왔는지 맥락을 이해하려 한 점이 좋았다

---

## 개선점

### 상황 1.
- **문제**: Normalization 기법들(Batch, Layer, Instance, Group)의 차이가 모호하다
- **원인**: 각 기법이 어떤 축을 기준으로 정규화하는지 시각적으로 비교하지 않았다
- **액션플랜**: 4가지 Normalization의 연산 축을 다이어그램으로 그려서 비교하기

---

## 배운 점

### 상황 1. CNN의 구성 요소
- **배움**: CNN은 Convolution Layers, Pooling Layers, Fully-Connected Layers, Normalization Layers, Dropout, Activation Functions로 구성된다.
- **의미**: CNN의 전체 구조를 이해하는 데 기본이 되는 구성 요소들을 정리해 두면 아키텍처를 분석할 때 각 레이어의 역할을 빠르게 파악할 수 있다.

### 상황 2. Normalization
- **배움**: 평균 = 0, 표준편차 = 1로 데이터를 정규화하는 과정이다. 이후 감마값과 베타값으로 위치가 조정되며 이 값은 Back Propagation에 의해 조정된다. 종류는 다음과 같다.
  - BN(Batch Norm): 하나의 채널에 대해 미니배치 단위(여러 장의 이미지)로 정규화를 수행한다
  - LN(Layer Norm): 하나의 이미지에 대해 여러 채널에 대해 정규화를 수행한다
  - Instance Norm: 한 이미지의 한 채널을 정규화한다
  - Group Norm: 한 이미지의 여러 채널을 정규화한다
- **의미**: 정규화 기법마다 어떤 축을 기준으로 연산하는지가 다르므로, 데이터 특성과 모델 구조에 맞는 Normalization을 선택하는 근거가 된다.

### 상황 3. Dropout
- **배움**: 모델 구조 중 일부 노드의 출력값을 0으로 조정하여 일반화 성능을 높이는 기법이다. 과적합 방지와 중복 특징 추출 방지에 효과가 있다. Drop 확률에 따라 노드의 출력값이 0이 될지 결정된다. BP 과정에서도 Drop된 노드와 연결된 모든 헤더 방향의 노드들이 BP 계산에 관여하지 못한다.
- **의미**: Dropout이 단순히 노드를 끄는 것이 아니라 BP 경로까지 차단한다는 점을 이해해야 정규화 효과의 원리를 정확히 파악할 수 있다.

### 상황 4. Activation Functions
- **배움**: 활성화 함수의 종류와 특성은 다음과 같다.
  - sigmoid: 심층 레이어에 걸쳐 반복적으로 사용할 경우 심층에서 기울기 값이 0에 근사하게 되므로 Back Propagation이 0에 수렴하며 학습이 제대로 되지 않는다. 이를 Vanishing Gradient라 한다.
  - ReLU: 비선형성을 확보하면서 미분이 매우 쉽다. 기울기 소실이 일어나지 않는다.
  - GeLU: x=0일 때 미분 가능성을 확보한 활성화 함수이다.
- **의미**: 활성화 함수 선택이 모델의 학습 안정성에 직접적인 영향을 미치므로, 각 함수의 장단점을 알아야 적절한 선택이 가능하다.

### 상황 5. VGGNet
- **배움**: 3x3 conv 연산을 3번 수행할 경우 최종 출력값에서 한 개의 픽셀이 입력 이미지에서 7x7의 수용영역을 대변한다. 7x7 conv layer 1개보다 3x3 conv layer 3개가 더 작은 가중치 개수를 가지면서 같은 크기의 수용영역을 가질 수 있으므로 계산적으로 이득이다.
- **의미**: 작은 필터를 여러 번 쌓는 것이 큰 필터 하나보다 효율적이라는 원리는 이후 다양한 CNN 아키텍처 설계의 기본 철학이 된다.

### 상황 6. ResNet
- **배움**: 일정 규모 이상의 CNN 네트워크는 깊어질수록 훈련 정확도가 오히려 떨어지는 경우가 생긴다. 이는 과적합이 아니고(훈련 정확도도 낮음), 깊은 네트워크가 얕은 네트워크보다 정확도가 높아지는 방향으로의 최적화(optimization)가 어렵기 때문이다. 따라서 1개 블록에 대한 출력값은 입력값 자체 X와 그 입력값에 대한 conv 연산 값의 합으로 이루어진다(Skip Connection). 기존 conv, pooling layer와 같은 연산들은 입력값 자체를 나타내기 매우 어렵기 때문에 이러한 구조가 학습에 더 큰 이점이 있다.
- **의미**: ResNet의 Skip Connection은 단순한 기법이 아니라 깊은 네트워크의 최적화 문제를 근본적으로 해결한 설계이므로, 딥러닝 아키텍처 이해의 핵심이 된다.

### 상황 7. Weight Initialization
- **배움**: 가중치의 초기값에 따라 모델 성능이 결정된다. 안정적인 모델 성능을 위해 Kaiming Initialization을 사용해야 한다. 이는 입력 차원 크기의 제곱근 2로 값을 초기화하는 방법이다.
- **의미**: 가중치 초기화가 학습의 출발점을 결정하므로, 적절한 초기화 전략 없이는 아무리 좋은 아키텍처도 제대로 학습되지 않을 수 있다.

### 상황 8. Data Preprocessing
- **배움**: 이미지 정규화를 각 채널마다 수행한다. 정규화 공식은 각 픽셀에서 해당 채널의 평균을 빼고 표준편차로 나누는 것이다.
- **의미**: 채널별 정규화는 모델이 색상 편향 없이 학습할 수 있도록 하는 기본 전처리 단계이다.

### 상황 9. Data Augmentation
- **배움**: 이미지 증강 기법에는 다음과 같은 것들이 있다.
  - 수평 뒤집기
  - 크기 조정 및 자르기
  - 색상 변경
  - 잘라내기
- **의미**: 데이터 증강은 제한된 데이터셋으로도 모델의 일반화 성능을 높일 수 있는 실용적인 방법이다.

### 상황 10. Transfer Learning
- **배움**: 가지고 있는 데이터가 모델 규모에 비해 작을 때, ImageNet과 같은 대규모 학습 모델의 마지막 헤더 부분만 재학습을 진행하는 방법이다. 모델의 나머지 네트워크 가중치들은 frozen(동결, 고정)해야 한다. 모델의 입력 인근은 일반적인 특징들을 잡아내며 헤더 부분으로 갈수록 데이터셋의 세부적인 특징들을 나타낸다.
- **의미**: Transfer Learning은 적은 데이터로도 높은 성능을 달성할 수 있게 해주는 핵심 기법이며, 실무에서 가장 많이 활용되는 학습 전략 중 하나이다.

### 상황 11. Hyperparameters 튜닝 전략
- **배움**: 하이퍼파라미터 튜닝 시 확인해야 할 사항은 다음과 같다.
  - 학습 손실이 0에 가까워지는지 확인한다
  - 작은 샘플에 대해 과적합되는지 확인한다
  - 적절한 Learning Rate를 찾는다
  - 정확도 그래프를 train, val 동시에 확인한다
- **의미**: 하이퍼파라미터 튜닝은 체계적인 점검 순서를 따라야 효율적이며, 학습 곡선을 통해 모델의 상태를 진단하는 습관이 중요하다.
